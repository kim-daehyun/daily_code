{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'G:\\\\내 드라이브\\\\revive_project\\\\squrt'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import patsy\n",
    "import numpy as np \n",
    "from statsmodels.regression.linear_model import OLS\n",
    "from scipy import stats\n",
    "import os\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "from numpy import genfromtxt\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report,accuracy_score\n",
    "from sklearn.metrics import recall_score, f1_score, precision_score, roc_auc_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import metrics \n",
    "\n",
    "from lightgbm import LGBMClassifier\n",
    "from lightgbm import plot_importance\n",
    "\n",
    "from xgboost.sklearn import XGBClassifier\n",
    "from xgboost import plot_importance \n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "\n",
    "\n",
    "import warnings\n",
    "\n",
    "import tensorflow as tf\n",
    "import datetime\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import funcs\n",
    "#import funcs_test\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "right_dir_path = 'G:\\\\내 드라이브\\\\revive_project\\\\squrt\\\\dataset\\\\squat'\n",
    "wrong_dir_path = 'G:\\\\내 드라이브\\\\revive_project\\\\squrt\\\\dataset\\\\cheating-손으로까딱'\n",
    "fake_dir_path = 'G:\\\\내 드라이브\\\\revive_project\\\\squrt\\\\dataset\\\\squat'\n",
    "\n",
    "right_raw_data = funcs.make_right_raw_data(right_dir_path)\n",
    "wrong_raw_data = funcs.make_wrong_raw_data(wrong_dir_path)\n",
    "fake_raw_data = funcs.make_fake_raw_data(fake_dir_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 정상 데이터 shape : (733, 123) \n",
      " 까딱 데이터 shape : (249, 123) \n",
      " 1s delayed 데이터 shape : (657, 123) \n",
      " 전체 데이터 shape : (1639, 123)\n",
      "\n",
      "정상 : 비정상(까딱+ 1s delayed) = 1 : 1.24\n"
     ]
    }
   ],
   "source": [
    "want_to_make = [right_raw_data, wrong_raw_data, fake_raw_data]\n",
    "\n",
    "raw_data = np.concatenate(want_to_make, axis =0)\n",
    "\n",
    "print( ' 정상 데이터 shape : {0} \\n 까딱 데이터 shape : {1} \\n 1s delayed 데이터 shape : {2} \\n 전체 데이터 shape : {3}\\n'\\\n",
    "      .format(right_raw_data.shape, wrong_raw_data.shape, fake_raw_data.shape ,raw_data.shape ))\n",
    "\n",
    "print( '정상 : 비정상(까딱+ 1s delayed) = {0} : {1}'\\\n",
    "      .format(1, round((wrong_raw_data.shape[0]+fake_raw_data.shape[0]) / right_raw_data.shape[0],2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 20\n",
    "bases = ['acc_x_','acc_y_','acc_z_','gyr_x_','gyr_y_','gyr_z_']\n",
    "cols = []\n",
    "#print('acc_x_' + str(0))\n",
    "#print('acc_x_' + str(1))\n",
    "\n",
    "for base in bases:\n",
    "    for i in range(batch_size):\n",
    "        cols.append(base+str(round(0.1*i,1)))\n",
    "        \n",
    "cols = cols + ['file_name', 'start_sec', 'label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(raw_data, columns = cols)\n",
    "\n",
    "X = df[df.columns[:-3]] \n",
    "y = df[df.columns[-1]] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2 , random_state = 0)\n",
    "\n",
    "y_test = y_test.astype('int')\n",
    "y_train = y_train.astype('int')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device mapping:\n",
      "/job:localhost/replica:0/task:0/device:XLA_CPU:0 -> device: XLA_CPU device\n",
      "/job:localhost/replica:0/task:0/device:GPU:0 -> device: 0, name: GeForce RTX 2070 SUPER, pci bus id: 0000:01:00.0, compute capability: 7.5\n",
      "/job:localhost/replica:0/task:0/device:XLA_GPU:0 -> device: XLA_GPU device\n",
      "\n",
      "shape: (500, 500)\n",
      "elapesed time: 0:00:00.026352\n"
     ]
    }
   ],
   "source": [
    "with tf.compat.v1.Session(config=tf.compat.v1.ConfigProto(log_device_placement=True)) as sess: \n",
    "\n",
    "    with tf.device('/device:GPU:0'):\n",
    "        \n",
    "        #실행할 코드\n",
    "        \n",
    "        random_matrix = tf.random.uniform(shape=shape, minval=0, maxval=1)\n",
    "        dot_operation = tf.matmul(random_matrix, tf.transpose(random_matrix))\n",
    "        sum_operation = tf.reduce_sum(dot_operation)\n",
    "        # Time the actual runtime of the operations\n",
    "        start_time = datetime.datetime.now()\n",
    "\n",
    "        result = sess.run(sum_operation)\n",
    "        elapsed_time = datetime.datetime.now() - start_time\n",
    "\n",
    "        print('shape' +\": \" + str(shape))\n",
    "        print(\"elapesed time: \" + str (elapsed_time))\n",
    "        \n",
    "\n",
    "        # PRINT ELAPSED TIME, SHAPE AND DEVICE USED  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device mapping:\n",
      "/job:localhost/replica:0/task:0/device:XLA_CPU:0 -> device: XLA_CPU device\n",
      "/job:localhost/replica:0/task:0/device:GPU:0 -> device: 0, name: GeForce RTX 2070 SUPER, pci bus id: 0000:01:00.0, compute capability: 7.5\n",
      "/job:localhost/replica:0/task:0/device:XLA_GPU:0 -> device: XLA_GPU device\n",
      "\n",
      "Fitting 5 folds for each of 9040 candidates, totalling 45200 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  18 tasks      | elapsed:    7.1s\n",
      "[Parallel(n_jobs=-1)]: Done 168 tasks      | elapsed:   52.8s\n",
      "[Parallel(n_jobs=-1)]: Done 418 tasks      | elapsed:  2.2min\n",
      "[Parallel(n_jobs=-1)]: Done 768 tasks      | elapsed:  4.0min\n",
      "[Parallel(n_jobs=-1)]: Done 1218 tasks      | elapsed:  6.3min\n",
      "[Parallel(n_jobs=-1)]: Done 1768 tasks      | elapsed:  9.3min\n",
      "[Parallel(n_jobs=-1)]: Done 2418 tasks      | elapsed: 12.7min\n",
      "[Parallel(n_jobs=-1)]: Done 3168 tasks      | elapsed: 16.6min\n",
      "[Parallel(n_jobs=-1)]: Done 4018 tasks      | elapsed: 21.0min\n",
      "[Parallel(n_jobs=-1)]: Done 4968 tasks      | elapsed: 25.9min\n",
      "[Parallel(n_jobs=-1)]: Done 6018 tasks      | elapsed: 31.2min\n",
      "[Parallel(n_jobs=-1)]: Done 7168 tasks      | elapsed: 37.2min\n",
      "[Parallel(n_jobs=-1)]: Done 8418 tasks      | elapsed: 43.7min\n",
      "[Parallel(n_jobs=-1)]: Done 9768 tasks      | elapsed: 50.6min\n",
      "[Parallel(n_jobs=-1)]: Done 11218 tasks      | elapsed: 58.0min\n",
      "[Parallel(n_jobs=-1)]: Done 12768 tasks      | elapsed: 66.0min\n",
      "[Parallel(n_jobs=-1)]: Done 14418 tasks      | elapsed: 74.5min\n",
      "[Parallel(n_jobs=-1)]: Done 16168 tasks      | elapsed: 83.5min\n",
      "[Parallel(n_jobs=-1)]: Done 18018 tasks      | elapsed: 93.1min\n",
      "[Parallel(n_jobs=-1)]: Done 19968 tasks      | elapsed: 103.2min\n",
      "[Parallel(n_jobs=-1)]: Done 22018 tasks      | elapsed: 113.9min\n",
      "[Parallel(n_jobs=-1)]: Done 24168 tasks      | elapsed: 125.0min\n",
      "[Parallel(n_jobs=-1)]: Done 26418 tasks      | elapsed: 136.6min\n",
      "[Parallel(n_jobs=-1)]: Done 28768 tasks      | elapsed: 148.9min\n",
      "[Parallel(n_jobs=-1)]: Done 31218 tasks      | elapsed: 161.8min\n",
      "[Parallel(n_jobs=-1)]: Done 33768 tasks      | elapsed: 175.3min\n",
      "[Parallel(n_jobs=-1)]: Done 36418 tasks      | elapsed: 189.5min\n",
      "[Parallel(n_jobs=-1)]: Done 39168 tasks      | elapsed: 204.1min\n",
      "[Parallel(n_jobs=-1)]: Done 42018 tasks      | elapsed: 219.2min\n",
      "[Parallel(n_jobs=-1)]: Done 44968 tasks      | elapsed: 234.8min\n",
      "[Parallel(n_jobs=-1)]: Done 45200 out of 45200 | elapsed: 236.0min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tuned hpyerparameters :(best parameters)  {'classifier': RandomForestClassifier(max_depth=11, max_features=16, min_samples_leaf=9,\n",
      "                       min_samples_split=4, n_estimators=80), 'classifier__max_depth': 11, 'classifier__max_features': 16, 'classifier__min_samples_leaf': 9, 'classifier__min_samples_split': 4, 'classifier__n_estimators': 80}\n",
      "train set_accuracy : 0.9115316518155169\n"
     ]
    }
   ],
   "source": [
    "with tf.compat.v1.Session(config=tf.compat.v1.ConfigProto(log_device_placement=True)) as sess: \n",
    "\n",
    "    with tf.device('/device:GPU:0'):\n",
    "        \n",
    "        #실행할 코드\n",
    "        \n",
    "        pipe = Pipeline([('classifier' , RandomForestClassifier())])\n",
    "\n",
    "        # Create param grid.\n",
    "\n",
    "        param_grid = [\n",
    "            {'classifier' : [LogisticRegression()],\n",
    "             'classifier__penalty' : ['l1', 'l2'],\n",
    "            'classifier__C' : np.logspace(-4, 4, 20),\n",
    "            'classifier__solver' : ['liblinear']},\n",
    "\n",
    "            {'classifier' : [RandomForestClassifier()],\n",
    "             'classifier__max_depth' : list(range(1,12,2)),\n",
    "             'classifier__min_samples_leaf' : list(range(3,12,2)),\n",
    "            'classifier__min_samples_split' : list(range(2,12,2)),\n",
    "            'classifier__n_estimators' : list(range(10,101,10)),   #랜덤 포레스트 안의 결정 트리 갯수 \n",
    "            'classifier__max_features' : list(range(6,32,5))}   #무작위로 선택할 Feature의 개수 \n",
    "            ]\n",
    "\n",
    "        # Create grid search object\n",
    "\n",
    "        clf = GridSearchCV(pipe, param_grid = param_grid, cv = 5, scoring='accuracy'\n",
    "                       ,verbose=True, n_jobs=-1)\n",
    "\n",
    "        # Fit on data\n",
    "\n",
    "        best_clf = clf.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "        print(\"tuned hpyerparameters :(best parameters) \",best_clf.best_params_)\n",
    "        print(\"train set_accuracy :\",best_clf.best_score_)\n",
    "\n",
    "        # PRINT ELAPSED TIME, SHAPE AND DEVICE USED  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = Pipeline([('classifier' , RandomForestClassifier())])\n",
    "\n",
    "# Create param grid.\n",
    "\n",
    "param_grid = [\n",
    "    {'classifier' : [LogisticRegression()],\n",
    "     'classifier__penalty' : ['l1', 'l2'],\n",
    "    'classifier__C' : np.logspace(-4, 4, 20),\n",
    "    'classifier__solver' : ['liblinear']},\n",
    "    \n",
    "    {'classifier' : [RandomForestClassifier()],\n",
    "     'classifier__max_depth' : list(range(1,12,2)),\n",
    "     'classifier__min_samples_leaf' : list(range(3,12,2)),\n",
    "    'classifier__min_samples_split' : list(range(2,12,2)),\n",
    "    'classifier__n_estimators' : list(range(10,101,10)),   #랜덤 포레스트 안의 결정 트리 갯수 \n",
    "    'classifier__max_features' : list(range(6,32,5))}   #무작위로 선택할 Feature의 개수 \n",
    "]\n",
    "\n",
    "# Create grid search object\n",
    "\n",
    "clf = GridSearchCV(pipe, param_grid = param_grid, cv = 5, scoring='accuracy'\n",
    "                   ,verbose=True, n_jobs=-1)\n",
    "\n",
    "# Fit on data\n",
    "\n",
    "best_clf = clf.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "print(\"tuned hpyerparameters :(best parameters) \",best_clf.best_params_)\n",
    "print(\"train set_accuracy :\",best_clf.best_score_)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
